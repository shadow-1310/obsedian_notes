## Topics

### Other IIT questions
- pandas merge, concat, join, combine methods
	- pd.merge / df.merge
		- on
		- how
	- pd.concat
		- stitches row-wise by default
		- objs
		- axis
		- join - inner/outer, defaut -> outer
	- df.join
		- on -> column name
		- how - left, right, inner
	- df.combine - Perform column-wise combine with another DataFrame.
		- takes a lambda function to combine columns of the dataframes
- generators
- gini coefficient and entropy calculation of a dataset
	- gini $$GI = 1-p_1^2-p_2^2-...$$
	- entropy $$H(s) = -p_+log(p_-)-p_+log(p_+)$$
- find MLE of mean and variance
	- [Bessel's correction](https://www.statisticshowto.com/bessels-correction/)
- statistical test
- questions on K-means and KNN
	- large K in KNN, high bias low variance
	- small K in KNN, low bias high variance
### Round1 - Maths & Stats(33.33%) - 30 mins
- [ ] Probability
	- [MCQs](https://www.analyticsvidhya.com/blog/2017/04/40-questions-on-probability-for-all-aspiring-data-scientists/)
- [ ] Probability Distributions
	- theoriticals
		- gaussian
	- practical - [ritvik video](https://www.youtube.com/watch?v=bOlEUWMKDek)
		- spike - limited by upper or lower bound
		- skewed 
		- bimodal - combination of 2 or more groups
		- pointy--- stable
		- noisy - check bin size or sample size
- [ ] Sampling
- [ ] Correlations
	- Pearson correlation
	- Rank Correlation
	- find slope of regression line from correlation and standard deviation values
		- ![[Pasted image 20231021224856.png]]
	- finding covariance and correlaton from mean(x) and mean(y)
- [ ] Hypothesis testing , [Zstatistics example](https://www.youtube.com/watch?v=2GU_R7G5m-8)
	- [Zdstatistics tutorial](https://www.youtube.com/watch?v=CJvmp2gx7DQ)
	- effect of s, n, delta on Power
		- s increase, power decreases
		- n increases, power increases
		- 
- [ ] Statistical significance
- [ ] CLT
- [ ] Paired Means Tests
- [ ] Baye's Theorem
- [ ] distance metric
	- for cat variable we need to use Hamming distance
- [ ] Linear Algebra
	- corelation and covariance
		- corelation is a standarized version of covariance
	- Covariance Matrix
		- it describes the shape of the distribution
	- [x] EigenValue Decomposition - [ritvikmath](https://www.youtube.com/watch?v=KTKAp9Q3yWg)
		- only applies to diagonal square matrix
		- eigenvector and eigenmatrix, $$Ax = \lambda x$$ 
		- usefull to reduce coputational cost of matrix power.
		- if we do matrix A to the power p, we need at least Log2(p) operations. while using eigen decomposition, we need only 2 matrix operations.
		- $$A^p = U\Lambda^pU^{-1}$$
		-  where A -> matrix, U -> eigenvector, lambda->eigenvalue
	- [ ] SVD - [ritvikmath](https://www.youtube.com/watch?v=HAJey9-Q8js)
		- it is useful for compression of big matrices, we can express a Matrix by the following.
		- $$M = U*\sigma *V^T, \newline $$
		- where 
			- M -> m * n
			- U, orthonormal matrix -> m * p
			- sigma, diagonal matrix -> p * p
			- V^T , orthonormal matrix-> p * n
		- right singular vectors, V
		- left singular vectors, U
		- singular values, sigma1, sigma2, sigmap
	 - [ ] Matrix Operations
		 - [ ] rank of a matrix
			 - if a matrix has rank p, it can be expressed as sum of p number of rank 1 matrices(atom matrices).
		 - [ ] jacobian
			 -  
		 - [ ] finding eigenvector and eigenvalue, [ritvik-tutorial](https://www.youtube.com/watch?v=glaiP222JWA)
		 - $$if\;det(A) = 0, A\;is\;not\;invertible$$
		$$det(A-\lambda*I)=0,\newline because\;(A-\lambda*I)*x=0, which\; gives (A-\lambda*I) as\;non determinant$$
		 - [ ] Matrix Norms -[ritvik](https://www.youtube.com/watch?v=DkyM93Wgh_0)
			 - $$||AB||_p \leq ||A||_p ||B||_p$$
			 - $$||A^k|| \geq Max|\lambda _i|^k$$
		 - [ ] singular values vs eigen values
### Round2 - Classical ML and DL(66.67%) - 45 mins
- [ ] Bagging 
	- decreases variance
- [ ] Boosting
	- decreases bias
	- other ensemble techniques are 
		- Extra Trees
		- Voting Classifier
		- Stacking
- [ ] Gradient Descent
- [ ] Decision Trees
- [ ] Linear and Logistic Regressions
	- response(y) and predictor(x1, x2, x3, ...)
- [ ] Random Forest
- [ ] Gradient Boosting Machines
- [ ] XGBoost
- [ ] LiteGBM
- [x] SVM and kernel trick
- [ ] Regularization
	- L1(Lasso)-makes weight 0
	- L2(Ridge) - shrinks weight to almost 0
- [x] Bias-Variance trade off
- [ ] Evaluation Metrics (precision, recall, f1, fpr etc.)
- [ ] confusion matrix
- [ ] missing data imputation
- [ ] matrix factorization
- [ ] density estimation and clustering techniques (KDE, K-Means)
- [analytics vidya mcq](https://www.analyticsvidhya.com/blog/2017/02/test-data-scientist-clustering/)
	- Kmeans - it can get stuck in local minima - Hard margin
		- Methods of initialization
			- Random
			- Forgy Method
	- Fuzzy Kmeans - soft margin
	- DBSCAN
	- GMM (Gaussian Mixture Model) [serrano](https://www.youtube.com/watch?v=q71Niz856KE) - Soft margin
		- overlapping clusters
		- mu
		- sigma
		- pi
	- EM algorithm(Expectation Maximization) - can stuck in local minima
	- [ritvik tutorial](https://www.youtube.com/watch?v=xy96ArOpntA)
		- we don't know 2 pieces of information at once
		- can be used for missing value imputations
- [ ] MAP
- [ ] MLE
- [ ] NLP basics [analytics vidya](https://www.analyticsvidhya.com/blog/2017/07/30-questions-test-data-scientist-natural-language-processing-solution-skilltest-nlp/)
	- how many bigram, trigram in a sentence?
	- polysemy - CNN
	- string matching
		- edit distance
		- metaphore
		- soundex
- [ ] TF-IDF
	- tf-idf calculation [ritvik video](https://www.youtube.com/watch?v=OymqCnh-APA)
	- TF(t,d)* IDF(t, D)
		- t -> term
		- d -> document
		- D -> corpus
- [ ] Embedding based approach ( Word2Vec, FastText, BERT)
	- Word2vec model, self-supervised model
		- [codeEmporium video](https://www.youtube.com/watch?v=9S0-OC4LFNo) [krishnaik video](https://www.youtube.com/watch?v=hKgUlpcZ1eI) ^401fd1
		- CBOW 
		- skipgram
		- cons
			- ![[Pasted image 20231020204628.png]]
	- GloVe
	- FastText
		- for morphologically rich languages 
		- considers sub-word information
- [ ] LSTM, CNN, RNN, transformers, GANs
- [ ] Activation functions
	- sigmoid (0-1)
	- tanh(-1-1), zero centred
- [ ] RBF Neural Network
	- uses rbf as activation at each neuron, have 3 parameters
		- unit centre(c) - K-means
		- unit width(r) - KNN
		- weight(w) - apply rbf(distance)

### Round3 - Programming Skills
- part 1- Data handling
	- argsort
	- np.where()
- part 2- ML basics

### Round4 - Case study