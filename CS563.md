## Project - Wikipedia Missing Pages
---
- https://stackoverflow.com/questions/10144820/get-the-html-of-the-javascript-rendered-page-after-interacting-with-it
- Objectives
	- if a named entity recently appears on news, we check
		- whether there exist a wiki page for it, if not we will suggest for its creation
		- when this page was first created and compare it with first news of that named entity
			- if wiki page was created after this named entity's first news, we'll suggest to add the missing information

#### Datasets
- Multistream data (01-03-2024)- not relevant for this project
	- contains current wikipedia entity page content along with latest date of creation.
	- Dataset description
		- xml format(.bz2)
		- 66 files (400-600MB) with accompanied by a index file(several MB)
		- download speed of 200-300 KBps
		- max 3 files at one time
		- 100 % download completed.
- Meta-history - During Week 1
	- contains complete edit/revision history of an entity page along with author 
	- Dataset size description
		- xml format(.7z)
		- 847 files (150-600 MB)
		- after uncompression file size becomes 34-40 GB
		- download speed of 200-300 KBps
		- almost 700 files downloaded
- News corpus with timestamps 
	- the paper we were following were using NYT dataset during 2001-2007
	- we downloaded N24 dataset (total 60k news articles of 24 categories)
	- dataset description
		- one category tag (total 24 categories)
		- one headline
		- one abstract
		- one article body
		- article url - we will parse timestamp from article url
		- one image
		- image caption
#### Obstacles
- Downloading large number of files
	- using a bash script to automate the process
- Requirement of large disk size to store all files (>3TB)
	- uncompress each file and parse it to make a smaller file, then delete the uncompressed big file.
- parsing each file - till now 160 files done
	- As each file has a size of almost 35GB, it doesn't fit in memory
	- so read it by chunk
	
#### Weekly Planning
- Week1
	- analyze lag with news article
- week2 
	- train a small model
- week3 
	- train final model



## Assignment 2 (Data Discovery and exploration)
### definitions
- data discovery - process of identifying datasets that may meet an information need
- data exploration - process of understanding properties of candidate datasets and relations among them.

## Aim
- addressing the requirement to identify several, potentially interrelated datasets within huge repositories, where the datasets within the repositories were likely produced independently of each other.
#### Points 
- According to survey 19% of data scientist's time is spent on discovering and selecting suitable dataset
- to support data scientist and data engineers in management of data repositories numerous metadata model has been proposed
- However, in a setting where there may be millions of datasets, and where these datasets may change rapidly [39], it is impractical to manually curate data catalogs with rich descriptions of all the available data and it can still be challenging to understand what data is available to support a specific task.
- support search over datasets that have been minimally annotated, automate annotation of datasets with terms from ontologies,

## Focus areas
#### Dataset search
- Given a repository of data, a search aims to retrieve datasets that, individually or together, satisfy an information need. Thus, given a collection of datasets C and a search request R,
- Search(C,R) -->  S
###### Dataset Driven search
- TUS - retrieve individual dataset that are union compatible with given dataset. 
	- considers similarity of values in different columns
	- word embedding of values from different columns
- D3L - Dataset Discovery in Data Lakes
	- less explicit focus on unionability
	- tries to find one or more datasets which are when joined are as similar to the given dataset.
	- uses ranking function that considers Euclidean distance between columns
- QCR - Quadrant Count Ratio (Join correlation search)
	- find top-k datasets that are joinable with given dataset and contains columns that are highly corelated with numerical columns in given dataset
	- 
#### Dataset Navigation
- Having identified a dataset, for example, as a result of a search process, it is often useful to be able to explore related datasets that may provide additional information or context.
- Thus, given a dataset d (a 1 , . . . , an ) and a collection of datasets C, there there may be an operation of the form
- $RealtedAttribute(d.a_i,C)\rightarrow S$

#### Data Annotation
- In a large repository, there are likely to be a variety of naming conventions, for example, because the data was originally produced by different publishers. nuisance, as it places the burden on the user of resolving inconsistencies as part of the process of selecting data for analysis.
- Data annotation is the process of associating intensional or extensional data items with a term from a vocabulary or a concept from an ontology.

#### Schema Inference
- Data integration can be associated with the notion of a global schemaâ€”a schema that an end user interacts with, and behind which the heterogeneity of different sources is hidden